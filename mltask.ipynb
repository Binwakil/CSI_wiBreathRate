{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some libraries we need in this part\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from cmath import pi\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import stft\n",
    "from scipy import signal\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6600, 245, 2, 2), (6000, 245, 2, 2), (6500, 245, 2, 2), (6400, 245, 2, 2), (6700, 245, 2, 2), (6900, 245, 2, 2), (3400, 245, 2, 2), (4300, 245, 2, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "with open('Data/trainset.pickle', 'rb') as handle:\n",
    "    trainset = pickle.load(handle)\n",
    "\n",
    "# Inspect the shapes of each CSI entry\n",
    "CSI_signals = trainset['CSI']\n",
    "shapes = [CSI.shape for CSI in CSI_signals]\n",
    "print(shapes)  # This will show the shapes of each CSI entry\n",
    "target_length = 6900 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Groundtruth Length: 66\n",
      "Processed Groundtruth Length: 6900\n",
      "Original Groundtruth Length: 60\n",
      "Processed Groundtruth Length: 6900\n",
      "Original Groundtruth Length: 65\n",
      "Processed Groundtruth Length: 6900\n",
      "Original Groundtruth Length: 64\n",
      "Processed Groundtruth Length: 6900\n",
      "Original Groundtruth Length: 67\n",
      "Processed Groundtruth Length: 6900\n",
      "Original Groundtruth Length: 69\n",
      "Processed Groundtruth Length: 6900\n",
      "Original Groundtruth Length: 2\n",
      "Skipping short groundtruth entry.\n",
      "Original Groundtruth Length: 2\n",
      "Skipping short groundtruth entry.\n",
      "CSI Signals Shape:  (8, 6900, 245, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "with open('Data/trainset.pickle', 'rb') as handle:\n",
    "    trainset = pickle.load(handle)\n",
    "\n",
    "# Extract CSI signals and groundtruth\n",
    "CSI_signals = trainset['CSI']\n",
    "groundtruth = trainset['groundtruth']\n",
    "\n",
    "# Function to process CSI signals\n",
    "def process_CSI_signals(CSI_signals, target_length):\n",
    "    processed_signals = []\n",
    "    for signal in CSI_signals:\n",
    "        signal = np.array(signal)\n",
    "        if signal.shape[0] > target_length:\n",
    "            # Truncate\n",
    "            signal_processed = signal[:target_length]\n",
    "        else:\n",
    "            # Pad\n",
    "            padding = target_length - signal.shape[0]\n",
    "            signal_processed = np.pad(signal, ((0, padding), (0, 0), (0, 0), (0, 0)), mode='constant', constant_values=0)\n",
    "        processed_signals.append(signal_processed)\n",
    "    return np.array(processed_signals)\n",
    "\n",
    "def process_groundtruth(groundtruth, target_length):\n",
    "    processed_groundtruth = []\n",
    "    for gt in groundtruth:\n",
    "        gt = np.array(gt)  # Ensure groundtruth entry is a numpy array\n",
    "        print(f'Original Groundtruth Length: {gt.shape[0]}')  # Debugging output\n",
    "        if gt.shape[0] > target_length:\n",
    "            # Truncate\n",
    "            gt_processed = gt[:target_length]\n",
    "        elif gt.shape[0] < target_length and gt.shape[0] > 2:\n",
    "            # Pad only if greater than 2\n",
    "            padding = target_length - gt.shape[0]\n",
    "            gt_processed = np.pad(gt, (0, padding), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            print(\"Skipping short groundtruth entry.\")\n",
    "            continue  # Skip entries that are too short\n",
    "        print(f'Processed Groundtruth Length: {gt_processed.shape[0]}')  # Debugging output\n",
    "        processed_groundtruth.append(gt_processed)\n",
    "    return np.array(processed_groundtruth)\n",
    "\n",
    "# Process the CSI signals and groundtruth\n",
    "CSI_signals_processed = process_CSI_signals(CSI_signals, target_length)\n",
    "groundtruth_processed = process_groundtruth(groundtruth, target_length)\n",
    "\n",
    "# Check the shapes\n",
    "print(\"CSI Signals Shape: \", CSI_signals_processed.shape)\n",
    "# print(\"Groundtruth Shape: \", groundtruth_processed.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Process the CSI signals and groundtruth\n",
    "# CSI_signals_processed = process_CSI_signals(CSI_signals, target_length)\n",
    "# groundtruth_processed = process_groundtruth(groundtruth, target_length)\n",
    "\n",
    "\n",
    "# # Check the shapes\n",
    "# print(\"Groundtruth Shape: \", CSI_signals_processed.shape)\n",
    "# print(\"Groundtruth Shape: \", groundtruth_processed.shape)\n",
    "\n",
    "# Now you can proceed with your deep learning model setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: dict_keys(['ex_name', 'CSI', 'CSI_timestamp', 'CSI_info', 'groundtruth'])\n",
      "Sample CSI shape: 8 Entries\n",
      "Sample Groundtruth shape: 8 Entries\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "with open('Data/trainset.pickle', 'rb') as handle:\n",
    "    trainset = pickle.load(handle)\n",
    "\n",
    "# Print the keys\n",
    "print(\"Keys in the dataset:\", trainset.keys())\n",
    "\n",
    "# Check the shape and structure of CSI and groundtruth\n",
    "CSI_sample = trainset['CSI']\n",
    "groundtruth_sample = trainset['groundtruth']\n",
    "\n",
    "print(\"Sample CSI shape:\", len(CSI_sample), \"Entries\")\n",
    "print(\"Sample Groundtruth shape:\", len(groundtruth_sample), \"Entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSI Entry 0 shape: (6600, 245, 2, 2)\n",
      "Groundtruth Entry 0 length: 66\n",
      "CSI Entry 1 shape: (6000, 245, 2, 2)\n",
      "Groundtruth Entry 1 length: 60\n",
      "CSI Entry 2 shape: (6500, 245, 2, 2)\n",
      "Groundtruth Entry 2 length: 65\n",
      "CSI Entry 3 shape: (6400, 245, 2, 2)\n",
      "Groundtruth Entry 3 length: 64\n",
      "CSI Entry 4 shape: (6700, 245, 2, 2)\n",
      "Groundtruth Entry 4 length: 67\n",
      "CSI Entry 5 shape: (6900, 245, 2, 2)\n",
      "Groundtruth Entry 5 length: 69\n",
      "CSI Entry 6 shape: (3400, 245, 2, 2)\n",
      "Groundtruth Entry 6 length: 2\n",
      "CSI Entry 7 shape: (4300, 245, 2, 2)\n",
      "Groundtruth Entry 7 length: 2\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape and content of each CSI entry and corresponding ground truth\n",
    "for i in range(len(CSI_sample)):\n",
    "    print(f\"CSI Entry {i} shape: {CSI_sample[i].shape}\")  # Print the shape of each CSI entry\n",
    "    print(f\"Groundtruth Entry {i} length: {len(groundtruth_sample[i])}\")  # Print the length of each ground truth entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: dict_keys(['ex_name', 'CSI', 'CSI_timestamp', 'CSI_info', 'groundtruth'])\n",
      "Number of CSI Entries: 8\n",
      "CSI Entry 0 shape: (6600, 245, 2, 2)\n",
      "CSI Entry 1 shape: (6000, 245, 2, 2)\n",
      "CSI Entry 2 shape: (6500, 245, 2, 2)\n",
      "CSI Entry 3 shape: (6400, 245, 2, 2)\n",
      "CSI Entry 4 shape: (6700, 245, 2, 2)\n",
      "CSI Entry 5 shape: (6900, 245, 2, 2)\n",
      "CSI Entry 6 shape: (3400, 245, 2, 2)\n",
      "CSI Entry 7 shape: (4300, 245, 2, 2)\n",
      "Number of Groundtruth Entries: 8\n",
      "Groundtruth Entry 0 length: 66\n",
      "Groundtruth Entry 1 length: 60\n",
      "Groundtruth Entry 2 length: 65\n",
      "Groundtruth Entry 3 length: 64\n",
      "Groundtruth Entry 4 length: 67\n",
      "Groundtruth Entry 5 length: 69\n",
      "Groundtruth Entry 6 length: 2\n",
      "Groundtruth Entry 7 length: 2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "with open('Data/trainset.pickle', 'rb') as handle:\n",
    "    trainset = pickle.load(handle)\n",
    "\n",
    "# Check the keys in the dataset\n",
    "print(\"Keys in the dataset:\", trainset.keys())\n",
    "\n",
    "# Extract the shapes of the CSI signals and groundtruth\n",
    "CSI_signals = trainset['CSI']\n",
    "groundtruth = trainset['groundtruth']\n",
    "\n",
    "# Check the number of entries and their shapes\n",
    "print(f\"Number of CSI Entries: {len(CSI_signals)}\")\n",
    "for idx, signal in enumerate(CSI_signals):\n",
    "    print(f\"CSI Entry {idx} shape: {signal.shape}\")\n",
    "\n",
    "print(f\"Number of Groundtruth Entries: {len(groundtruth)}\")\n",
    "for idx, gt in enumerate(groundtruth):\n",
    "    print(f\"Groundtruth Entry {idx} length: {len(gt)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Groundtruth Length: 66\n",
      "Padding groundtruth entry to length: 69\n",
      "Processed Groundtruth Length: 69\n",
      "Original Groundtruth Length: 60\n",
      "Padding groundtruth entry to length: 69\n",
      "Processed Groundtruth Length: 69\n",
      "Original Groundtruth Length: 65\n",
      "Padding groundtruth entry to length: 69\n",
      "Processed Groundtruth Length: 69\n",
      "Original Groundtruth Length: 64\n",
      "Padding groundtruth entry to length: 69\n",
      "Processed Groundtruth Length: 69\n",
      "Original Groundtruth Length: 67\n",
      "Padding groundtruth entry to length: 69\n",
      "Processed Groundtruth Length: 69\n",
      "Original Groundtruth Length: 69\n",
      "Processed Groundtruth Length: 69\n",
      "Original Groundtruth Length: 2\n",
      "Skipping groundtruth entry due to insufficient length.\n",
      "Original Groundtruth Length: 2\n",
      "Skipping groundtruth entry due to insufficient length.\n",
      "Processed Groundtruth Shape:  torch.Size([6, 69])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35856/4264095175.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  gt_tensor = torch.tensor(gt, dtype=torch.float32)  # Convert to tensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "# Function to process groundtruth using PyTorch\n",
    "def process_groundtruth_pytorch(groundtruth, target_length):\n",
    "    processed_groundtruth = []\n",
    "    \n",
    "    for gt in groundtruth:\n",
    "        gt_tensor = torch.tensor(gt, dtype=torch.float32)  # Convert to tensor\n",
    "        print(f'Original Groundtruth Length: {gt_tensor.shape[0]}')  # Debugging output\n",
    "\n",
    "        # Check the length of groundtruth entry\n",
    "        if gt_tensor.shape[0] == 2:  # Skip entries with length 2\n",
    "            print('Skipping groundtruth entry due to insufficient length.')\n",
    "            continue\n",
    "        \n",
    "        if gt_tensor.shape[0] < target_length:\n",
    "            # Pad with zeros if the entry is shorter than the target length\n",
    "            padding = target_length - gt_tensor.shape[0]\n",
    "            gt_processed = torch.nn.functional.pad(gt_tensor, (0, padding), value=0)  # Pad with zeros\n",
    "            print(f'Padding groundtruth entry to length: {target_length}')\n",
    "        else:\n",
    "            # If the entry is longer than the target length, truncate it\n",
    "            gt_processed = gt_tensor[:target_length]\n",
    "\n",
    "        print(f'Processed Groundtruth Length: {gt_processed.shape[0]}')  # Debugging output\n",
    "        processed_groundtruth.append(gt_processed)\n",
    "\n",
    "    # Stack the tensors only if we have processed groundtruth entries\n",
    "    if processed_groundtruth:  # Check if the list is not empty\n",
    "        return torch.stack(processed_groundtruth)  # Stack the list into a tensor\n",
    "    else:\n",
    "        raise ValueError(\"No valid groundtruth entries were processed.\")\n",
    "\n",
    "# Process the groundtruth\n",
    "groundtruth_processed = process_groundtruth_pytorch(trainset['groundtruth'], 69)\n",
    "\n",
    "# Check the final shape\n",
    "print(\"Processed Groundtruth Shape: \", groundtruth_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSI Signal Shape: torch.Size([6600, 245, 2, 2])\n",
      "Padding CSI signal to length: 6900\n",
      "Processed CSI Signal Shape: torch.Size([6600, 245, 302, 2])\n",
      "Original CSI Signal Shape: torch.Size([6000, 245, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35856/657104476.py:9: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:305.)\n",
      "  signal_tensor = torch.tensor(signal, dtype=torch.float32)  # Keep complex parts separate if needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding CSI signal to length: 6900\n",
      "Processed CSI Signal Shape: torch.Size([6000, 245, 902, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [6600, 245, 302, 2] at entry 0 and [6000, 245, 902, 2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(processed_signals) \u001b[38;5;28;01mif\u001b[39;00m processed_signals \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Process the CSI signals\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m CSI_signals_processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_CSI_signals_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSI_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6900\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Check the final shape\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed CSI Signals Shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, CSI_signals_processed\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m CSI_signals_processed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mprocess_CSI_signals_pytorch\u001b[0;34m(CSI_signals, target_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Optional: If memory is tight, you can process in smaller batches here\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed_signals) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# Example: process every 2 signals\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# Convert to tensor and clear the list to free up memory\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_signals\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert list to tensor if needed\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         processed_signals \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Clear to free memory\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Ensure any remaining signals are processed\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [6600, 245, 302, 2] at entry 0 and [6000, 245, 902, 2] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to process CSI signals using PyTorch in batches\n",
    "def process_CSI_signals_pytorch(CSI_signals, target_length):\n",
    "    processed_signals = []\n",
    "    \n",
    "    for signal in CSI_signals:\n",
    "        # Convert to tensor with float32 (or float16 if needed)\n",
    "        signal_tensor = torch.tensor(signal, dtype=torch.float32)  # Keep complex parts separate if needed\n",
    "        print(f'Original CSI Signal Shape: {signal_tensor.shape}')  # Debugging output\n",
    "        \n",
    "        if signal_tensor.shape[0] > target_length:\n",
    "            # Truncate\n",
    "            signal_processed = signal_tensor[:target_length]\n",
    "        else:\n",
    "            # Pad\n",
    "            padding = target_length - signal_tensor.shape[0]\n",
    "            # Pad with zeros only on the first dimension\n",
    "            signal_processed = torch.nn.functional.pad(signal_tensor, (0, 0, 0, padding), value=0)  # Pad with zeros\n",
    "            print(f'Padding CSI signal to length: {target_length}')\n",
    "        \n",
    "        processed_signals.append(signal_processed)\n",
    "        print(f'Processed CSI Signal Shape: {signal_processed.shape}')  # Debugging output\n",
    "        \n",
    "        # Clear the original signal tensor to free up memory\n",
    "        del signal_tensor\n",
    "\n",
    "        # Optional: If memory is tight, you can process in smaller batches here\n",
    "        if len(processed_signals) >= 2:  # Example: process every 2 signals\n",
    "            # Convert to tensor and clear the list to free up memory\n",
    "            torch.stack(processed_signals)  # Convert list to tensor if needed\n",
    "            processed_signals = []  # Clear to free memory\n",
    "\n",
    "    # Ensure any remaining signals are processed\n",
    "    if processed_signals:\n",
    "        torch.stack(processed_signals)\n",
    "\n",
    "    # Stack all processed signals into a single tensor\n",
    "    return torch.stack(processed_signals) if processed_signals else None\n",
    "\n",
    "# Process the CSI signals\n",
    "CSI_signals_processed = process_CSI_signals_pytorch(CSI_signals, 6900)\n",
    "\n",
    "# Check the final shape\n",
    "print(\"Processed CSI Signals Shape: \", CSI_signals_processed.shape if CSI_signals_processed is not None else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m CSI_data \u001b[38;5;241m=\u001b[39m trainset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSI\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Extract the CSI data from your dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Normalize data (GANs usually perform better with normalized inputs)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m CSI_data \u001b[38;5;241m=\u001b[39m CSI_data \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSI_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Flatten the data for input into GAN (reshape as needed)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m CSI_flattened \u001b[38;5;241m=\u001b[39m CSI_data\u001b[38;5;241m.\u001b[39mreshape(CSI_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape each CSI sample into a 1D vector\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load CSI data (Amplitude + Phase)\n",
    "CSI_data = trainset['CSI']  # Extract the CSI data from your dataset\n",
    "\n",
    "# Normalize data (GANs usually perform better with normalized inputs)\n",
    "CSI_data = CSI_data / np.max(np.abs(CSI_data))\n",
    "\n",
    "# Flatten the data for input into GAN (reshape as needed)\n",
    "CSI_flattened = CSI_data.reshape(CSI_data.shape[0], -1)  # Reshape each CSI sample into a 1D vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess Amplitude and Phase\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m CSI_amplitude \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSI_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m CSI_phase \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mangle(CSI_data)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Combine features (weighted sum or concatenation)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Preprocess Amplitude and Phase\n",
    "CSI_amplitude = np.abs(CSI_data)\n",
    "CSI_phase = np.angle(CSI_data)\n",
    "\n",
    "# Combine features (weighted sum or concatenation)\n",
    "combined_features = CSI_amplitude + 0.5 * CSI_phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
